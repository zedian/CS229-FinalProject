{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1d63b48ecfce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\shap\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# explainers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kernel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKernel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mKernelExplainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSampling\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mSamplingExplainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\shap\\explainers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_permutation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPermutation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_partition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPartition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_gpu_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPUTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_exact\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExact\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\shap\\explainers\\_permutation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpartition_tree_shuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaskedModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\shap\\explainers\\_explainer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmaskers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\shap\\maskers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_masker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMasker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tabular\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndependent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImpute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_image\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_text\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_fixed\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFixed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\shap\\maskers\\_image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# TODO: heapq in numba does not yet support Typed Lists so we can move to them yet...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNumbaPendingDeprecationWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNumbaPendingDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numba.core'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import numba\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv(\"model_v2.csv\")\n",
    "computer = [col for col in df.columns if \"computer\" in col.lower()]\n",
    "df = df.drop(computer, axis=1)\n",
    "df.columns = [c.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"<\",\"\") for c in df.columns]\n",
    "df.columns = [re.sub(r'[^a-zA-Z0-9]', '', s) for s in df.columns]\n",
    "print([col for col in df.columns if \"computer\" in col.lower()])\n",
    "# print(len(set(df.columns)))\n",
    "# print(len(df.columns))\n",
    "\n",
    "# df = df.set_index(\"Name\")\n",
    "deaths = [col for col in df.columns if \"covid\" in col.lower()]\n",
    "tot_pop = [col for col in df.columns if \"total\" in col.lower()][0]\n",
    "y = df[deaths[0]]\n",
    "y_cases = df[deaths[1]]\n",
    "# Normalized y\n",
    "# y_lab = y[labels[0]]\n",
    "# df = df[df[labels[11]].isna() == False]\n",
    "df = df[y.isna() == False]\n",
    "y_lab = y[y.isna() == False]\n",
    "x = df.drop(deaths, axis=1)\n",
    "# x = x.drop(tot_pop, axis=1)\n",
    "bad_col = [c for c in x.columns if x[c].isna().sum() > 10]\n",
    "x = x.drop(bad_col, axis=1)\n",
    "print(y_lab.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_T, X_test, y_T, y_test = train_test_split(x, y_lab, test_size=0.15, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "y_pred_train = lm.predict(X_train)\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"Root Mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.2f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'alphas': [[0.1], [0.2], [1.0], [2.0]],\n",
    "    'max_iter': [10000, 100000],\n",
    "    'l1_ratio': [0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "}\n",
    "\n",
    "# Initialize the linear regression model with L1 and L2 regularization (Elastic Net)\n",
    "enet_model = ElasticNetCV()\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for the linear regression model\n",
    "clf = GridSearchCV(enet_model, parameters, cv=4, scoring='neg_root_mean_squared_error', n_jobs=8)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "# # Calculate the R^2 statistic\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Print the mean squared error and the R^2 statistic\n",
    "print(\"Root Mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"ROot Mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "\n",
    "print(\"Coefficient of determination: %.4f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.4f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "    return mean_squared_error(x, y, squared=False)\n",
    "xgb = xgboost.XGBRegressor(n_estimators=1000, max_depth=3, learning_rate=0.01, tree_method=\"hist\", eval_metric=rmse)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "y_pred_train = xgb.predict(X_train)\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"Root Mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.2f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVR(C=0.4, epsilon=0.3, kernel=\"sigmoid\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "y_pred_train = svm.predict(X_train)\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"Root Mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.2f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a LightGBM dataset for training\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Create a LightGBM dataset for validation\n",
    "val_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Define the parameters for LightGBM\n",
    "parameters = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 30,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 3,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(parameters, train_data, valid_sets=val_data, num_boost_round=500, early_stopping_rounds=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"Root Mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.2f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_train)\n",
    "shap.plots.beeswarm(shap_values, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LightGBM dataset for training\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Create a LightGBM dataset for validation\n",
    "val_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Define the parameters for LightGBM\n",
    "parameters = {\n",
    "    'num_leaves': [10, 30, 50],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'feature_fraction': [0.8, 0.9],\n",
    "    'bagging_freq': [5, 7]\n",
    "}\n",
    "model = lgb.LGBMRegressor(objective=\"regression\", metric=\"rmse\")\n",
    "\n",
    "clf = GridSearchCV(model, parameters, scoring='r2', cv=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "# Calculate the mean squared error\n",
    "\n",
    "print(\"Root mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"Root mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "\n",
    "print(\"Coefficient of determination: %.4f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.4f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBRegressor(n_estimators=100, max_depth=10, tree_method=\"hist\")\n",
    "\n",
    "# Set up the parameters for the XGBoost model\n",
    "parameters = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(xgb, parameters, scoring='explained_variance', cv=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "# Calculate the mean squared error\n",
    "\n",
    "print(\"Root mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"Root mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "\n",
    "print(\"Coefficient of determination: %.4f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.4f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = RandomForestRegressor()\n",
    "\n",
    "# Set up the parameters for the XGBoost model\n",
    "parameters = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(rdf, parameters, scoring='r2', cv=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "# Calculate the mean squared error\n",
    "\n",
    "print(\"Root mean squared error: %.9f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"Root mean squared error train: %.9f\" % mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "\n",
    "print(\"Coefficient of determination: %.4f\" % r2_score(y_test, y_pred))\n",
    "print(\"Coefficient of determination train: %.4f\" % r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
